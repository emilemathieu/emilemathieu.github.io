<!DOCTYPE HTML>
<!--
    Spatial by TEMPLATED
    templated.co @templatedco
    Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
        <head>
        <title>Article test | Emile Mathieu - Blog</title>
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <meta name="description" content="Works" />
        <meta name="keywords" content="works,Emile Mathieu,Mathieu emile,emilemathieu" />
        <link rel="shortcut icon" href="Images/enpc_favicon.ico" type="image/vnd.microsoft.icon" />
        <!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
        <!--<script src="js/jquery.min.js"></script>
        <script src="js/skel.min.js"></script>
        <script src="js/skel-layers.min.js"></script>
        <script src="js/init.js"></script>
        -->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/skel/2.2.1/skel.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/skel-layers/2.0.2/skel-layers.min.js"></script>
        <script src="js/init.js"></script>
        
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-90612705-1', 'auto');
          ga('send', 'pageview');

        </script>
        
        <noscript>
            <!-- <link rel="stylesheet" href="css/skel.css" /> -->
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/skel/2.2.1/skel.css" />
            <link rel="stylesheet" href="css/style.css" />
            <link rel="stylesheet" href="css/style-xlarge.css" />
        </noscript>
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <!-- For offline development: -->
        <!--<script type="text/javascript" src="./MathJax-master/MathJax.js?config=TeX-AMS_HTML-full"></script>-->
        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
          TeX: { equationNumbers: { autoNumber: "AMS" } }
        });
        </script>
        <style type="text/css">
            canvas { border: 1px solid white; }
        </style>
    </head>
    <body class="landing">

          <!-- Header -->
            <header id="header" class="alt">
                <nav id="share" style="left: 1.25em;">
                    <ul class="icons">
                        <li style="margin-left:1em"><a href="https://github.com/emilemathieu" target="_blank" class="icon fa-github"><span class="label">Github</span></a></li>
                        <li style="margin-left:1em"><a href="https://fr.linkedin.com/in/emilemathieu" target="_blank" class="icon  fa-linkedin-square"><span class="label">Linkedin</span></a></li>
                    </ul>
                </nav>
                <!--<h1><strong><a href="index.html">Emile Mathieu</a></strong></h1>-->
                <nav id="nav">
                    <ul>
                        <li><a href="index.html">Home</a></li>
                        <!-- <li><a href="software.html">Software</a></li> -->
                        <li><a href="works.html">Works</a></li>
                        <li><a href="misc.html">Misc</a><li>
                        <li><strong><a href="blog/index.html">Blog</a></strong><li>
                        <li><a href="contact.html">Contact</a></li>
                    </ul>
                </nav>
            </header>

        <!-- Banner -->
            <section id="banner">
                <h2>Emile Mathieu</h2>
            </section>


        <!-- uncertainty -->
            <section id="uncertainty" class="wrapper">
                <div class="container blog" style="max-width: 800px;">

                    <header class="major special">
                        <h2>Test blog post</h2>
                        <p>July 3rd, 2015</p>
                    </header>

                    <p>
                        <div class="social" style="text-align: center;">
                            <span class="twitter">
                                <a href="https://twitter.com/share" class="twitter-share-button"{count} data-url="http://emilemathieu.fr/blogpost1.html" data-size="default">Tweet</a>
                            </span>
                            <span class="google">
                                <div class="g-plusone" data-href="http://emilemathieu.fr/blogpost1.html"></div>
                            </span>
                            <span class="Facebook">
                                <div id="fb-root"></div>
                                <div class="fb-share-button" data-href="http://emilemathieu.fr/blogpost1.html" data-layout="button_count" style="height: 21px; width: 85px" allowTransparency="true"></div>
                            </span>
                        </div>
                    </p>

                    <h3>Dropout and Deep models</h3>
                    <p style="display:none">
                    $
                    \newcommand{\R}{\mathbb{R}}
                    \newcommand{\N}{\mathcal{N}}
                    \newcommand{\cL}{\mathcal{L}}
                    \newcommand{\cO}{\mathcal{O}}
                    \newcommand{\svert}{~|~}
                    \newcommand{\td}{\text{d}}
                    \newcommand{\f}{\mathbf{f}}
                    \newcommand{\x}{\mathbf{x}}
                    \newcommand{\Bb}{\mathbf{b}}
                    \newcommand{\sBb}{\mathtt{b}}
                    \newcommand{\bx}{\overline{\x}}
                    \newcommand{\bb}{\overline{b}}
                    \newcommand{\y}{\mathbf{y}}
                    \newcommand{\z}{\mathbf{z}}
                    \newcommand{\w}{\mathbf{w}}
                    \newcommand{\W}{\mathbf{W}}
                    \newcommand{\ba}{\mathbf{a}}
                    \newcommand{\m}{\mathbf{m}}
                    \newcommand{\ls}{\mathbf{l}}
                    \newcommand{\bL}{\mathbf{L}}
                    \newcommand{\A}{\mathbf{A}}
                    \newcommand{\X}{\mathbf{X}}
                    \newcommand{\Y}{\mathbf{Y}}
                    \newcommand{\F}{\mathbf{F}}
                    \newcommand{\I}{\mathbf{I}}
                    \newcommand{\M}{\mathbf{M}}
                    \newcommand{\p}{\mathbf{p}}
                    \newcommand{\bp}{\overline{\p}}
                    \newcommand{\bz}{\mathbf{0}}
                    \newcommand{\bepsilon}{\text{$\epsilon$}}
                    \newcommand{\bgamma}{\text{$\gamma$}}
                    \newcommand{\s}{\mathbf{s}}
                    \newcommand{\Unif}{\text{Unif}}
                    \newcommand{\bo}{\text{$\omega$}}
                    \newcommand{\bsigma}{s}
                    \newcommand{\bSigma}{\text{$\Sigma$}}
                    \newcommand{\bmu}{\text{$\mu$}}
                    \newcommand{\bphi}{\text{$\phi$}}
                    \newcommand{\K}{\mathbf{K}}
                    \newcommand{\Kh}{\widehat{\mathbf{K}}}
                    \newcommand{\Cov}{\text{Cov}}
                    \newcommand{\Var}{\text{Var}}
                    \newcommand{\tr}{\text{tr}}
                    \newcommand{\tdet}{\text{det}}
                    \newcommand{\diag}{\text{diag}}
                    \newcommand{\KL}{\text{KL}}
                    \newcommand{\ind}{\mathds{1}}
                    \newcommand{\bc}{\mathbf{c}}
                    \newcommand{\reg}{\eta}
                    \newcommand{\wd}{\lambda}
                    \newcommand{\argmin}{\text{argmin}}
                    $
                    </p>
                    <p>Let's go over the dropout network model quickly for a <i>single hidden layer</i> and the task of regression. We denote by $\W_1, \W_2$ the weight matrices connecting the first layer to the hidden layer and connecting the hidden layer to the output layer respectively. These linearly transform the layers' inputs before applying some element-wise non-linearity $\sigma(\cdot)$.
                    We denote by $\Bb$ the biases by which we shift the input of the non-linearity. 
                    We assume the model outputs $D$ dimensional vectors while its input is $Q$ dimensional vectors, with $K$ hidden units. We thus have $\W_1$ is a $Q \times K$ matrix, $\W_2$ is a $K \times D$ matrix, and $\Bb$ is a $K$ dimensional vector. A standard network would output $\widehat{\y} = \sigma(\x \W_1 + \Bb) \W_2$ given some input $\x$.</p>
                    <p>Dropout is a technique used to avoid over-fitting in these simple networks &ndash; a situation where the model can't generalise well from its training data to the test data. It was introduced several years ago by <a href="http://arxiv.org/pdf/1207.0580.pdf" target="_blank">Hinton et al.</a> and studied more extensively in (<a href="http://jmlr.org/papers/v15/srivastava14a.html" target="_blank">Srivastava et al.</a>). To use dropout we sample two binary vectors $\sBb_1, \sBb_2$ of dimensions $Q$ and $K$ respectively. The elements of vector $\sBb_i$ take value 1 with probability $0 \le p_i \le 1$ for $i = 1,2$. Given an input $\x$, we set $1 - p_1$ proportion of the elements of the input to zero: $\x \sBb_1$ (to keep notation clean we will write $\sBb_1$ when we mean $\diag(\sBb_1)$ with the $\diag(\cdot)$ operator mapping a vector to a diagonal matrix whose diagonal is the elements of the vector).
                    The output of the first layer is given by $\sigma(\x \sBb_1 \W_1 + \Bb)$, in which we randomly set $1 - p_2$ proportion of the elements to zero, and linearly transform to give the dropout model's output $\widehat{\y} = \sigma(\x \sBb_1 \W_1 + \Bb) \sBb_2 \W_2$. 
                    We repeat this for multiple layers. </p>
                    <p>
                        <!-- <div class="image captioned row 100% uniform special">
                            <div class="3u 0u$(medium)"></div>
                            <div class="6u 12u$(medium)"><span class="image captioned fit"><img src="blog_images/do_net.jpg" alt=""></span></div>
                            <div class="12u"><h5>Dropout is applied by simply dropping-out units at random with a certain probability during training.</h5></div>
                        </div> -->
                    </p>
                    <p>To use the network for regression we might use the euclidean loss,
                    $
                    E = \frac{1}{2N}
                    \sum_{n=1}^N ||\y_n - \widehat{\y}_n||^2_2
                    $
                    where $\{\y_1, ..., \y_N\}$ are $N$ observed outputs, and $\{\widehat{\y}_1, ..., \widehat{\y}_N\}$ are the outputs of the model with corresponding observed inputs $\{ \x_1, ..., \x_N \}$. During optimisation a regularisation term is often added.
                    We often use $L_2$ regularisation weighted by some weight decay $\wd$ (alternatively, the derivatives might be scaled), resulting in a minimisation objective (often referred to as cost),
                    \begin{align*} \label{eq:L:dropout}
                    \cL_{\text{dropout}} := E + \wd \big( &||\W_1||^2_2 + ||\W_2||^2_2 \notag\\
                    &+ ||\Bb||^2_2 \big).
                    \end{align*}
                    Note that optimising this objective is equivalent to scaling the derivatives of the cost by the learning rate and the derivatives of the regularisation by the weight decay after back-propagation, and this is how this optimisation is often implemented.
                    We sample new realisations for the binary vectors $\sBb_i$ for every input point and every forward pass thorough the model (evaluating the model's output), and use the same values in the backward pass (propagating the derivatives to the parameters to be optimised $\W_1,\W_2,\Bb$). The dropped weights $\sBb_1\W_1$ and $\sBb_2\W_2$ are often scaled by $\frac{1}{p_i}$ to maintain constant output magnitude. At test time we do not sample any variables and simply use the full weights matrices $\W_1,\W_2,\Bb$. 
                    This model can easily be generalised to multiple layers and classification.
                    There are many open source packages implementing this model (such as <a href="http://deeplearning.net/software/pylearn2/" target="_blank">Pylearn2</a> and <a href="http://caffe.berkeleyvision.org/" target="_blank">Caffe</a>). <!-- We'll use SnippyHolloW's <a href="https://github.com/SnippyHolloW/DL4H/blob/master/dnn.py" target="_blank">simple implementation</a> below. --></p>


                    <p>As you'd expect, it is as easy to implement these two equations. <!--<a>Here</a><!-- ToDo -> you'll find SnippyHolloW's adapted Python code to generate samples from the network at test time. We then--> We can use the following few lines of Python code to get the predictive mean and uncertainty:
                    <div class="row 100% uniform">
                        <div class="2u 0u$(medium)"></div><div class="8u 12u$(medium)">
                            <span class="image captioned fit special"><pre><code>probs = []
for _ in xrange(T):
    probs += [model.output_probs(input_x)]
predictive_mean = numpy.mean(prob, axis=0)
predictive_variance = numpy.var(prob, axis=0)
tau = l**2 * (1 - model.p) / (2 * N * model.weight_decay)
predictive_variance += tau**-1
</code></pre><h5>Python code to obtain predictive mean and uncertainty from dropout networks</h5></span>
                        </div><div class="2u 0u$(medium)"></div>
                    </div>
                    </p>


<style type="text/css">
/* This gets Google to fall into place */
.social {
    font-size: 1px;
    text-align: right;
}

/* This gets Facebook to fall into place */
/*.social iframe {
    vertical-align: middle;
}*/

/* Set an optional width for your button wrappers */
.social span {
    display: inline-block;
    /*width: 110px;*/
}

.social .twitter {
    margin-right: 6px;
}
.social .twitter a {
    margin-left: 5px;
    margin-top: 2px;
    color: #4e5665;
    font-size: 11px;
    line-height: 18px;
}
.social .google {
    width: 83px;
}
/*.social .Facebook {
    width: 85px;
}
.social .Facebook div span {
    vertical-align: middle !important;
}*/
</style>
<div id="fb-root"></div>
<script>(function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.3";
    fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>

<script src="https://apis.google.com/js/platform.js" async defer>
  {lang: 'en-GB'}
</script>

<script>window.twttr = (function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0],
    t = window.twttr || {};
  if (d.getElementById(id)) return t;
  js = d.createElement(s);
  js.id = id;
  js.src = "https://platform.twitter.com/widgets.js";
  fjs.parentNode.insertBefore(js, fjs);
  t._e = [];
  t.ready = function(f) {
    t._e.push(f);
  };
  return t;
}(document, "script", "twitter-wjs"));</script>

<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

var disqus_config = function () {
this.page.url = 'http://emilemathieu.fr';  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = 'blogpost1'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://http-emilemathieu-fr.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>

    </body>
</html>